{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"stops is a set containing all the english stopwords and punctuations containing all the punctuations and later\n",
    "adding all the punctuations to the stops set...\"\"\"\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "stops.update(punctuations)\n",
    "\n",
    "\"\"\"clean_words accepts all the words in the document,,,,firstly converting all the words to lower case....\n",
    "secondly removing stop words from it...and after that removing numeric characters \n",
    "from the list ..if there is any..and finally returning the whole list keeping / as a delimiter with \n",
    "all the cleaned words...\"\"\"\n",
    "\n",
    "def clean_words(words):\n",
    "    new_words = [w.lower() for w in words]  #converting all words to lower case\n",
    "    new_words2 = [w for w in new_words if not w in stops]  #removing stop words\n",
    "    new_words3 = [w.strip() for w in new_words2 if w.isalpha()]  #removing numeric values\n",
    "    final_set = '/'.join(new_words3)\n",
    "    return final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" file_locations returns the list of dictionaries where each dictionary contains location of each document..\n",
    "keys in the dictionary are root folder of file....filename....to which category it belongs ...and ....path\n",
    "of that file....\"\"\"\n",
    "\n",
    "\n",
    "def file_locations(folder_name):\n",
    "    all_paths = []\n",
    "    walk_func = os.walk(folder_name, topdown = False)\n",
    "    for head, directory, file_list in walk_func :#traversing through each folders\n",
    "        for file in file_list:  #traversing  through each files inside that folder\n",
    "            dict_doc = {}   #creating dictionary for each file\n",
    "            dir_name = head.split('/')[-1]\n",
    "            dict_doc['head'] = head  \n",
    "            dict_doc['path'] = head+\"/\"+file\n",
    "            dict_doc['file_name'] = file\n",
    "            dict_doc['class_name'] = dir_name\n",
    "            all_paths.append(dict_doc)\n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" clean_docs accepts all the paths of the documents...opens them...and it tokenizes all whole content \n",
    "of the document and calls the clean_words function for removing useless words and after that it creates a new\n",
    "file with the same name at different location...and writes all the cleaned words within that new file which is returned \n",
    "by the clean_words function\"\"\"\n",
    "\n",
    "\n",
    "def clean_docs(document_paths) :\n",
    "    i = 0\n",
    "    print(\"Processing..\",end=\"\")\n",
    "    for doc_path in document_paths:   #traversing through ecah file\n",
    "        path = doc_path['path']\n",
    "        with codecs.open(path, 'r', encoding='utf-8',errors='ignore') as fdata:  #opening the file\n",
    "            text = fdata.read()  #reading the data within that file\n",
    "            tokens = word_tokenize(text)  #tokenizing the whole content within that file\n",
    "            clean_tokens = clean_words(tokens)  #removing the useless words\n",
    "            i += 1\n",
    "            \n",
    "            clean_folder = doc_path['head'].replace('20_newsgroups','cleaned_content/20_newsgroups')  #renaming the name of root folder of that file\n",
    "            os.makedirs(clean_folder,exist_ok = True)   #creating cleaned_content folder if it is not there..\n",
    "            \n",
    "            clean_files_location = clean_folder + \"/\" +doc_path['file_name'] + '.txt'  #allocating new path to the file...\n",
    "            \n",
    "            with open(clean_files_location, 'wb') as data :  #creating a new file with the same name but at different location\n",
    "                data.write(bytes(clean_tokens,'utf8')) #writing the cleaned words within that file...\n",
    "                data.close()   #closing the file....\n",
    "            \n",
    "            if i % 400 == 0 :\n",
    "                print(\"..\",end=\"\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_paths = file_locations(\"20_newsgroups\")   #extracting location of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...it may take some time...\n",
      "Processing....................................................................................................Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" calling clean_docs function which creates a new folder by the name cleaned_content..which contains \n",
    "all the documents by the same name containing only the useful words separated by delimiter / \"\"\"\n",
    "\n",
    "print(\"Please wait...it may take some time...\")\n",
    "clean_docs(document_paths)  #creating new files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paths = file_locations(\"cleaned_content\")  #extracting location of cleaned files....\n",
    "\n",
    "\"\"\"created a dataframe containg 2 columns class-name i.e to which category doc belongs and path of that document\"\"\"\n",
    "\n",
    "main_df = pd.DataFrame(new_paths)   \n",
    "main_df.drop([\"head\",\"file_name\"],axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Provided all the categories a unique numeric value...i.e Y and all the paths of the documents is X ...after\n",
    "that splited X and Y using sklearn into test and train..\"\"\"\n",
    "\n",
    "class_list = list(set(main_df['class_name'].values))   #extracting names of all the classes...\n",
    "class_dict = { class_list[i]:i for i in range(len(class_list))}   #assigning numbers to each class name\n",
    "main_df['class_value'] = [class_dict[name] for name in main_df['class_name'].values]   #adding new column containing the values that is provided to each classes\n",
    "\n",
    "X = main_df['path']  #pulling out X FROM THE dataest\n",
    "Y = main_df['class_value']  #pulling out Y from the dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0) #spliting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dictionary...........................................Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" selecting_words function scans the content of all the new documents and creates a dictionary out of that...\n",
    "where all the words are keys and frequency of that word is the value..\n",
    "AFTER THAT....created 2 lists out of that dictionary \n",
    "first ones name is selected_features containing only those words whose frequency is 16 or more than 16\n",
    "last feature name of Selected_feature list is class_num which is appended as extra at last ..will be used later in\n",
    "representing category of the document....\n",
    "Second ones name is Selected_featues_freq containing frequency of all those words....\n",
    "\n",
    "and it finally returns the selected_features list..\"\"\"\n",
    "\n",
    "def selecting_words(X_train):\n",
    "    main_vocab = {}   #dictionary contaning frequency of each word\n",
    "    i = 0\n",
    "    print(\"Creating Dictionary.\",end=\"\")\n",
    "    for path in X_train:   #traversing through each training file\n",
    "        with codecs.open(path, 'r', encoding='utf-8',errors='ignore') as fdata:   #opening the file\n",
    "            text = fdata.read().split('/')    \n",
    "            i += 1\n",
    "            for word in text:  #traversing through each words within that file...\n",
    "                main_vocab[word] = main_vocab.get(word,0) + 1   \n",
    "            if i % 700 == 0 :\n",
    "                print(\"..\",end=\"\")\n",
    "    \n",
    "    sorted_vocab = sorted(main_vocab, key = main_vocab.get , reverse=True)   #sorting the dictionary w.r.t values\n",
    "    selected_features = []\n",
    "    selected_features_freq = []\n",
    "    for word in sorted_vocab:  #traversing through each word and removing those having frequency less than 16\n",
    "        if main_vocab[word] < 16:\n",
    "            continue\n",
    "        selected_features.append(word)\n",
    "        selected_features_freq.append(main_vocab[word])\n",
    "    selected_features.append(\"class_num\")  #adding one more column...which will contain the class to which the document belongs to...\n",
    "    print(\"Done!\")\n",
    "    return selected_features\n",
    "\n",
    "selected_features = selecting_words(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dataset_frequency accepts selected_features...X_data(document paths)...and Y-data(document categories)...\n",
    "this function actually creates a dataset...where columns are all the selected features ...and each rows represents \n",
    "each document....rows contains the frequency of each feature for a particular document...and last column of the \n",
    "dataset depicts the category of that document ....and finally returning the dataset\"\"\"\n",
    "\n",
    "def dataset_frequency(selected_features,x_data,y_data):\n",
    "    \n",
    "    selected_words_dictionary = {word : True for word in selected_features}   #creating a dictionary and assigning true values to each word...to be used later..\n",
    "    file_locations = x_data.values\n",
    "    file_class = y_data.values\n",
    "    frequency_set = []\n",
    "    print(\"Creating Dataset.\",end=\"\")\n",
    "    a = 0\n",
    "    for file in file_locations:   #traversing through each files..\n",
    "        freq_file = []\n",
    "        freq_dict = {}\n",
    "        \n",
    "        with codecs.open(file, 'r', encoding='utf-8',errors='ignore') as fdata:\n",
    "            text = fdata.read().split('/')  #reading the data\n",
    "            for word in text:  #traversing through the content\n",
    "                freq_dict[word] = freq_dict.get(word,0) + 1\n",
    "            for word in selected_features:   #traversing through all the features\n",
    "                if word not in freq_dict:   #if feature is not in dictionary assigning zzero value to that feature and adding that key to dict\n",
    "                    freq_dict[word] = 0\n",
    "            if a % 300 == 0:\n",
    "                print(\".\",end=\"\")\n",
    "        freq_file = [freq_dict[word] for word in selected_features]  #creating the final list ...in proper sequence as the sequence of words in selected features list..\n",
    "        freq_file[-1] = file_class[a]  #appending the class value to the last feature column that was assigned earlier\n",
    "        frequency_set.append(freq_file)   #appending the list to the frequency set...\n",
    "        a += 1\n",
    "    print(\"Wait...\",end=\"\")\n",
    "    df = pd.DataFrame(frequency_set, columns = selected_features)   #creating a final dataset with that frequency set..\n",
    "    print(\"Done!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataset...................................................Done!\n"
     ]
    }
   ],
   "source": [
    "training_dataset = dataset_frequency(selected_features,X_train,Y_train)  #creating dataset for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataset..................Wait...Done!\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = dataset_frequency(selected_features,X_test,Y_test)  #creating dataset for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training_dataset.drop([\"class_num\"],axis = 1) #separating x_train out of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = training_dataset[\"class_num\"]   #separating y_train out of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = testing_dataset.drop([\"class_num\"],axis = 1)  #separating x_test out of the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = testing_dataset[\"class_num\"]  #separating y_test out of testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the fit function is used for creating a dictionary where each category will be a key and each key  will be having \n",
    "an another dictionary...which will consist of all the features as keys..and values of those keys will\n",
    "be the frequency of occurance of that feature within that particular category or class...\n",
    "..this function  accepts training dataset...and returns the dictionary...\"\"\"\n",
    "\n",
    "def fit(df):\n",
    "    prob_dictionary = {}  \n",
    "    for target in set(df[\"class_num\"]):  #traversing through all the classes\n",
    "        prob_dictionary[target] = {}   #creating dict for each\n",
    "        temp = df[df[\"class_num\"] == target]   #creating a new dataset from the main one ...which contains rows of that particular class only..\n",
    "        total_count = 0   \n",
    "        for word in range(len(df.columns)-1):   #traversing through each features\n",
    "            prob_dictionary[target][df.columns[word]] = temp[df.columns[word]].sum()  #adding all the frequency under that feature and assigning it to dictionary\n",
    "            total_count += prob_dictionary[target][df.columns[word]]   #maintaining total_count of words under that class\n",
    "        prob_dictionary[target][\"total_count\"] = total_count   #adding total count to the dictionary\n",
    "    return prob_dictionary   \n",
    "\n",
    "prob_dictionary = fit(training_dataset)  #calling the above function...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"this function probablity...accepts the dictionary that is returned by above function and the x-train...\n",
    "It returns a dictionary where each categories or classes to which each documnets belongs to.... depicts keys..\n",
    "....and value of each key is list ........where each element of that list contains probablity of occurance \n",
    "of the feature within that particular class...\"\"\"\n",
    "\n",
    "def probablity(prob_dictionary,x_train):\n",
    "    probablity_dict = {}\n",
    "    no_of_features = len(x_train.columns)   \n",
    "    for target in prob_dictionary.keys():  #traversing through each classes ...\n",
    "        words_probablity_array = []   #creating a list for each\n",
    "        for j in x_train.columns:   #traversing through each feature... \n",
    "            prob_word = prob_dictionary[target][j]+1/(prob_dictionary[target][\"total_count\"]+no_of_features)   #calculating probablity of that feature...\n",
    "            words_probablity_array.append(prob_word)  \n",
    "        probablity_dict[target] = np.log(np.array(words_probablity_array))  #coverting the list to numpy array ...finding out the log of each elements and assigning it to the dictionary under that particular class..\n",
    "    return probablity_dict\n",
    "\n",
    "probablity_dict = probablity(prob_dictionary,x_train)   #calling the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this function predict2 returns the class to which a document belongs to\"\"\"\n",
    "\n",
    "def predict2(doc,prob_dictionary,probablity_dict,y_train):\n",
    "    class_counts = y_train.value_counts().to_dict()   #calculating the occurance of each class in y_train...\n",
    "    total_class_counts = y_train.shape[0]  #calculating total class counts...\n",
    "    max_class = 0  #for storing the class_name\n",
    "    max_value = -1.334   #for storing the probablity of being to that class\n",
    "    test_array = np.array(doc)\n",
    "    for target in probablity_dict:   #traversing through each classes \n",
    "        probablity = test_array * probablity_dict[target]   #multiplying the probablity array and frequency array...\n",
    "        final_proba = probablity.sum() * (class_counts[target]/total_class_counts)   #calculating the final probablity of being to that particular class for that document..\n",
    "        if max_value < final_proba:   #comparing the probablity with max one...if it is greater than max...assigning the new value to max...\n",
    "            max_value = final_proba\n",
    "            max_class = target\n",
    "    return max_class    #returns the class with max probablity for that particular document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"this function is used for predicting the values for x_test...\"\"\"\n",
    "\n",
    "def predict(prob_dictionary,probablity_dict,x_test,y_train):\n",
    "    Y_predict = []\n",
    "    a = 0\n",
    "    for doc in x_test.values:   #traversing through each rows of x_test\n",
    "        Y_predict.append(predict2(doc,prob_dictionary,probablity_dict,y_train))    #calling the above function...and appending the returned value class to the y_predict..\n",
    "        a += 1\n",
    "        if a % 100 == 0:\n",
    "            print(a,\"Files Processed\")\n",
    "    return Y_predict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Files Processed\n",
      "200 Files Processed\n",
      "300 Files Processed\n",
      "400 Files Processed\n",
      "500 Files Processed\n",
      "600 Files Processed\n",
      "700 Files Processed\n",
      "800 Files Processed\n",
      "900 Files Processed\n",
      "1000 Files Processed\n",
      "1100 Files Processed\n",
      "1200 Files Processed\n",
      "1300 Files Processed\n",
      "1400 Files Processed\n",
      "1500 Files Processed\n",
      "1600 Files Processed\n",
      "1700 Files Processed\n",
      "1800 Files Processed\n",
      "1900 Files Processed\n",
      "2000 Files Processed\n",
      "2100 Files Processed\n",
      "2200 Files Processed\n",
      "2300 Files Processed\n",
      "2400 Files Processed\n",
      "2500 Files Processed\n",
      "2600 Files Processed\n",
      "2700 Files Processed\n",
      "2800 Files Processed\n",
      "2900 Files Processed\n",
      "3000 Files Processed\n",
      "3100 Files Processed\n",
      "3200 Files Processed\n",
      "3300 Files Processed\n",
      "3400 Files Processed\n",
      "3500 Files Processed\n",
      "3600 Files Processed\n",
      "3700 Files Processed\n",
      "3800 Files Processed\n",
      "3900 Files Processed\n",
      "4000 Files Processed\n",
      "4100 Files Processed\n",
      "4200 Files Processed\n",
      "4300 Files Processed\n",
      "4400 Files Processed\n",
      "4500 Files Processed\n",
      "4600 Files Processed\n",
      "4700 Files Processed\n",
      "4800 Files Processed\n",
      "4900 Files Processed\n",
      "5000 Files Processed\n"
     ]
    }
   ],
   "source": [
    "y_predict = predict(prob_dictionary,probablity_dict,x_test,y_train)   #calling the above function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---Results-----Manually performed code.. \n",
      "Precision....Recall...F1-score for the predicted data\n",
      "-----------------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.54      0.57       243\n",
      "          1       0.86      0.66      0.75       261\n",
      "          2       0.89      0.83      0.86       270\n",
      "          3       0.56      0.78      0.65       258\n",
      "          4       0.78      0.61      0.69       249\n",
      "          5       0.78      0.93      0.85       238\n",
      "          6       0.88      0.89      0.89       246\n",
      "          7       0.99      0.83      0.91       283\n",
      "          8       0.89      0.95      0.92       271\n",
      "          9       0.83      0.76      0.79       256\n",
      "         10       0.97      0.89      0.93       249\n",
      "         11       0.92      0.51      0.65       261\n",
      "         12       0.73      0.95      0.82       237\n",
      "         13       0.97      0.95      0.96       264\n",
      "         14       0.80      0.81      0.81       236\n",
      "         15       0.68      0.75      0.71       253\n",
      "         16       0.77      0.71      0.74       240\n",
      "         17       0.87      0.98      0.92       212\n",
      "         18       0.73      0.84      0.78       233\n",
      "         19       0.70      0.88      0.78       240\n",
      "\n",
      "avg / total       0.81      0.80      0.80      5000\n",
      "\n",
      "Confusion Matrix for the predicted data....\n",
      "-----------------------------------------------------\n",
      "[[131   0   0  35   0   1   0   0   0   7   0   0   0   5   0   0   0   0\n",
      "   63   1]\n",
      " [  2 171   3   3   0  16   4   0   0   3   0   1  24   0   5  14   8   0\n",
      "    0   7]\n",
      " [  0   3 225  19   1   4   1   1   2   2   1   4   5   0   2   0   0   0\n",
      "    0   0]\n",
      " [ 22   0   0 202   0   4   1   0  16   7   1   0   4   0   0   0   0   0\n",
      "    0   1]\n",
      " [  0   3   0   2 153   2   3   0   0   0   0   0   4   0   6  20  20   0\n",
      "    1  35]\n",
      " [  1   1   0   6   0 222   1   0   3   1   0   0   0   0   0   1   0   2\n",
      "    0   0]\n",
      " [  1   0   0   3   0   1 219   0   1   4   0   0   8   1   0   3   0   1\n",
      "    1   3]\n",
      " [  1   3  13   8   0   4   5 236   2   6   0   2   0   0   1   2   0   0\n",
      "    0   0]\n",
      " [  0   0   0  10   0   0   0   0 258   1   0   0   0   0   0   0   0   0\n",
      "    2   0]\n",
      " [  9   0   0  43   1   0   0   0   1 195   1   0   5   0   0   0   0   0\n",
      "    1   0]\n",
      " [  1   0   1   2   0   3   3   0   0   0 221   0   1   0   0   1   0  16\n",
      "    0   0]\n",
      " [  2  10   9  11  11  16   4   1   2   3   2 132  12   0  15  14   7   5\n",
      "    1   4]\n",
      " [  1   0   0   2   1   1   0   0   0   3   0   0 226   0   0   1   0   0\n",
      "    0   2]\n",
      " [  6   0   0   2   0   0   0   0   1   1   0   0   0 252   0   1   0   0\n",
      "    1   0]\n",
      " [  1   7   0   2   7   1   0   0   0   1   0   1   5   0 192   7   7   0\n",
      "    0   5]\n",
      " [  1   0   1   4   4   7   4   0   2   0   0   1   7   1   2 189   7   3\n",
      "    1  19]\n",
      " [  2   0   0   1  15   1   1   0   2   1   0   2   7   0  16   7 170   2\n",
      "    0  13]\n",
      " [  0   0   0   3   0   1   0   0   0   0   1   0   0   0   0   0   0 207\n",
      "    0   0]\n",
      " [ 33   0   0   1   0   0   0   0   1   1   0   0   1   1   0   0   0   0\n",
      "  195   0]\n",
      " [  1   0   0   1   3   2   2   0   0   0   0   0   2   0   0  16   1   1\n",
      "    0 211]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\" ---Results-----Manually performed code.. \")\n",
    "print(\"Precision....Recall...F1-score for the predicted data\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(classification_report(y_test,y_predict)) \n",
    "print(\"Confusion Matrix for the predicted data....\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(confusion_matrix(y_test,y_predict))  #for seeing how many documents are predicted correctly...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING MULTINOMIAL NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" extracting the values from the datasets for input in Multinomial Naive Bayes classifier...\"\"\"\n",
    "x_train2 = x_train.values   \n",
    "x_test2 = x_test.values\n",
    "y_train2 = y_train.values\n",
    "y_test2 = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0.01)    \n",
    "clf.fit(x_train2, y_train2)  #fiting the data within the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(x_test2)    #predicting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results using Multinomial Naive Bayes classifier\n",
      "Precision....Recall...F1-score for the predicted data\n",
      "-----------------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.55      0.59       243\n",
      "          1       0.83      0.85      0.84       261\n",
      "          2       0.89      0.91      0.90       270\n",
      "          3       0.74      0.72      0.73       258\n",
      "          4       0.89      0.55      0.68       249\n",
      "          5       0.94      0.92      0.93       238\n",
      "          6       0.96      0.89      0.92       246\n",
      "          7       0.94      0.94      0.94       283\n",
      "          8       0.94      0.94      0.94       271\n",
      "          9       0.83      0.89      0.86       256\n",
      "         10       0.97      0.96      0.96       249\n",
      "         11       0.80      0.84      0.82       261\n",
      "         12       0.98      0.93      0.96       237\n",
      "         13       0.96      0.98      0.97       264\n",
      "         14       0.78      0.88      0.83       236\n",
      "         15       0.72      0.78      0.75       253\n",
      "         16       0.69      0.81      0.75       240\n",
      "         17       0.96      0.98      0.97       212\n",
      "         18       0.75      0.83      0.78       233\n",
      "         19       0.83      0.85      0.84       240\n",
      "\n",
      "avg / total       0.85      0.85      0.85      5000\n",
      "\n",
      "Confusion Matrix for the predicted data....\n",
      "-----------------------------------------------------\n",
      "[[133   0   0  29   0   0   0   1   0  10   0   0   0   9   0   0   0   0\n",
      "   60   1]\n",
      " [  0 222   3   0   0   0   0   0   0   0   0   7   0   0   8  11   9   0\n",
      "    0   1]\n",
      " [  0   3 246   2   0   1   0   3   0   2   0   7   0   0   2   2   1   0\n",
      "    0   1]\n",
      " [ 26   0   0 186   0   3   1   2  13  20   3   0   2   0   2   0   0   0\n",
      "    0   0]\n",
      " [  2   4   0   0 136   1   0   0   0   0   1   6   0   0  14  19  46   0\n",
      "    0  20]\n",
      " [  1   2   0   2   0 220   1   0   0   2   0   1   0   0   0   6   0   1\n",
      "    2   0]\n",
      " [  0   3   1   2   0   3 220   2   1   3   0   3   0   0   1   4   1   1\n",
      "    0   1]\n",
      " [  0   1   9   0   0   0   1 266   0   2   0   4   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   9   0   0   0   0 255   3   0   0   0   0   1   0   0   0\n",
      "    2   0]\n",
      " [  5   0   0  16   1   0   0   1   0 228   2   1   1   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   2   0   0   0   2   1   0   0 239   1   0   0   0   1   0   3\n",
      "    0   0]\n",
      " [  0  10  10   1   2   0   0   3   0   0   1 218   0   0   5   3   5   2\n",
      "    1   0]\n",
      " [  1   2   0   1   1   1   2   0   0   3   0   0 221   0   0   4   0   0\n",
      "    0   1]\n",
      " [  3   0   0   1   0   0   0   0   1   0   0   0   0 258   1   0   0   0\n",
      "    0   0]\n",
      " [  0   9   1   0   2   0   0   0   0   0   0   6   0   0 208   1   7   0\n",
      "    0   2]\n",
      " [  0   7   1   0   3   3   2   1   0   0   0   6   0   0   5 197  13   0\n",
      "    1  14]\n",
      " [  0   5   2   1   5   0   1   0   0   1   0   9   0   0  16   5 194   1\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   0   1   0   0   1   1   0   0   0   0   0 208\n",
      "    0   0]\n",
      " [ 36   0   0   0   0   0   0   0   0   1   0   0   1   2   0   0   0   0\n",
      "  193   0]\n",
      " [  1   1   0   0   2   2   0   1   0   0   0   1   0   0   2  22   4   1\n",
      "    0 203]]\n"
     ]
    }
   ],
   "source": [
    "print(\" Results using Multinomial Naive Bayes classifier\")\n",
    "print(\"Precision....Recall...F1-score for the predicted data\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(classification_report(y_test2,Y_pred))\n",
    "print(\"Confusion Matrix for the predicted data....\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(confusion_matrix(y_test2,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------:CONCLUSION:-------------------------------------\n",
    "-----------------------------------:COMPARISON :------------------------------------\n",
    "As we know that...The above task is performed in two methods....firstly..using manually performed code...and secondly using multinomial naive bayes...\n",
    "RESULTS:------\n",
    "**1st...USING ---Manually performed code...:\n",
    "Precision - 0.81,\n",
    "Recall - 0.80,\n",
    "F1-Score - 0.80,\n",
    "**2nd...USING ---Inbuilt Multinomial Naive Bayes classifier...:\n",
    "Precision - 0.85,\n",
    "Recall - 0.85,\n",
    "F1-Score - 0.85,\n",
    "\n",
    "**From the results we can see that inbuilt multinomial naive bayes is providing better results than the manually written code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
